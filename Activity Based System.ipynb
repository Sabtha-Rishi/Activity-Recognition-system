{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc05b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling as pp\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe16203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_data combines all the data present in seperate folders inside the main folder as a single pandas DataFrame\n",
    "#Required parameter : Main folder location\n",
    "\n",
    "def get_data(folder_location):\n",
    "    \n",
    "    def file_to_df(location):\n",
    "        df = pd.read_csv(location, skiprows=4, low_memory=False )\n",
    "        df[\"Time\"] = df[\"# Columns: time\"]\n",
    "        df.drop(\"# Columns: time\", axis =1 , inplace = True)\n",
    "        return df\n",
    "\n",
    "    all_datas = []\n",
    "    pathss=[]\n",
    "    os.chdir(folder_location)\n",
    "    for i in os.listdir():\n",
    "        path = str(os.getcwd())+\"\\\\\"+str(i)\n",
    "        pathss.append(path)\n",
    "    \n",
    "    for path in pathss[1:]:\n",
    "        \n",
    "        try:\n",
    "            os.chdir(path)\n",
    "        except:\n",
    "            pass\n",
    "        for file in os.listdir():\n",
    "            try:\n",
    "                location = str(os.getcwd()+\"\\\\\"+str(file))\n",
    "                df = pd.read_csv(location, skiprows=4 )\n",
    "                df[\"Time\"] = df[\"# Columns: time\"]\n",
    "                df.drop(\"# Columns: time\", axis =1 , inplace = True)\n",
    "                df[\"Label\"]= location.split(sep=\"\\\\\")[-2]\n",
    "                \n",
    "                \n",
    "                all_datas.append(df)\n",
    "            except:\n",
    "                print(\"could not read the file in this location : \" + location)\n",
    "    \n",
    "    data = pd.concat(all_datas)\n",
    "            \n",
    "    return all_datas , data\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c653ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_data , df  = get_data(\"C:\\\\Users\\\\RISHI\\\\Desktop\\\\AReM\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ce77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(df).to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The returned data has read a invalid CSV file and hence resulting in null values\n",
    "#### Dropping duplicate values\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abded8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns starting with \"var\" means the variance according to the dataset description\n",
    "# They contain zero values. Since variance cannot be zero, it is replaced with mean or median\n",
    "\n",
    "df[\"var_rss12\"].replace(0, df[\"var_rss12\"].mean(), inplace=True)\n",
    "df[\"var_rss13\"].replace(0, df[\"var_rss13\"].mean(), inplace=True)\n",
    "df[\"var_rss23\"].replace(0, df[\"var_rss23\"].mean(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0fd47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Box Plot for visualizing  outliers\n",
    "fig ,ax  = plt.subplots(figsize = (10,10))\n",
    "sns.boxplot(data = df.drop(\"Time\",axis=1) , ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the data for prevention\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5931a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#copying the data for prevention\n",
    "df2 = df.copy()\n",
    "def drop_outliers(data,column,threshold):\n",
    "    \n",
    "    q3 = np.percentile(sorted(data[column]), 75)\n",
    "    q1 = np.percentile(sorted(data[column]), 25)\n",
    "    \n",
    "    IQR = q3 - q1\n",
    "    \n",
    "    ul = q3+threshold*IQR\n",
    "    ll = q1-threshold*IQR\n",
    "    \n",
    "    \n",
    "    return data[((data[column]<ul) & (data[column]>ll))]\n",
    "    \n",
    "    \n",
    "#df2 = drop_outliers(df2,\"avg_rss13\",1.6)\n",
    "#df2 = drop_outliers(df2,\"avg_rss23\",1.5)\n",
    "#df2 = drop_outliers(df2,\"avg_rss12\",1.5) \n",
    "#df2 = drop_outliers(df2,\"var_rss12\",0.5)\n",
    "#df2 = drop_outliers(df2,\"var_rss13\",1.4)\n",
    "#df2 = drop_outliers(df2,\"var_rss23\",1.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target and Feauture\n",
    "\n",
    "x = df2.drop(\"Label\", axis=1)\n",
    "y = df2[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "\n",
    "scalar = StandardScaler()\n",
    "x_scaled = scalar.fit_transform(x.copy())\n",
    "\n",
    "#Encoding Target Category\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(list(df2.Label.unique()))\n",
    "y = encoder.transform(y.copy())\n",
    "encoder.inverse_transform([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726babf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test split\n",
    "x_train , x_test, y_train, y_test = train_test_split(x_scaled,y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585140f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the heatmap of confusin matrix and the Report of the algorithm\n",
    "\n",
    "def performance(x_train,x_test, y_train, y_test, model):\n",
    "    \n",
    "    predicted = model.predict(x_test)\n",
    "    cm = confusion_matrix(y_test,predicted)\n",
    "\n",
    "    report = classification_report(y_test, predicted,output_dict=True )\n",
    "    \n",
    "    report_values =[]\n",
    "    for i in report.items():\n",
    "        report_values.append(i)\n",
    "        \n",
    "    return cm, report_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers = performance(x_train,x_test,y_train,y_test,logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when comparing the model performance with and without the outliers, the model with the outliers present performs better.\n",
    "model_with_outliers = performance(x_train,x_test,y_train,y_test,logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d463c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning \n",
    "\n",
    "\n",
    "param_grid = [{\"penalty\" : ['l1', 'l2', 'elasticnet'],\n",
    "               \"dual\" : [True, False],\n",
    "               \"tol\" : [0.0001,0.0002,0.0003],\n",
    "               \"C\" :[1.0, 1.5, 0.5],\n",
    "               \"intercept_scaling\" : [1,2],\n",
    "               \"solver\" : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "               \"max_iter\" : [100,150,200],\n",
    "               \"multi_class\" : ['auto', 'ovr', 'multinomial'],\n",
    "               \"warm_start\" : [True,False],\n",
    "               \"n_jobs\" : [-1]}]\n",
    "\n",
    "def fine_tuning(param_grid,estimator,n_iter):\n",
    "    \n",
    "    rdm_search = RandomizedSearchCV(estimator,param_distributions=param_grid,n_jobs=-1,n_iter=n_iter,cv=10)\n",
    "    \n",
    "    rdm_search.fit(x_scaled,y)\n",
    "    best_param = rdm_search.best_params_\n",
    "    best_score = rdm_search.best_score_\n",
    "    \n",
    "    return best_param , best_score\n",
    "    \n",
    "               \n",
    "estimator = LogisticRegression()\n",
    "fine_tuning(param_grid, estimator, 40)      \n",
    "\n",
    "#The Hypertuned model performs similar to the model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9668d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of the fine tuned model\n",
    "\n",
    "tuned_model = LogisticRegression(warm_start= True,\n",
    "  tol= 0.0003,\n",
    "  solver= 'saga',\n",
    "  penalty= 'l1',\n",
    "  n_jobs= -1,\n",
    "  multi_class= 'auto',\n",
    "  max_iter= 100,\n",
    "  intercept_scaling= 1,\n",
    "  dual= False,\n",
    "  C= 0.5,)\n",
    "\n",
    "tuned_model.fit(x_train,y_train)\n",
    "tuned_performance = performance(x_train,x_test,y_train,y_test,tuned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model as a pickle file\n",
    "file = \"Logistic_model.sav\"\n",
    "pickle.dump(tuned_model,open(file,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f169756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the data using the saved model\n",
    "saved_model = pickle.load(open(file,\"rb\"))\n",
    "model_prediction = saved_model.predict(x_test)\n",
    "\n",
    "model_prediction = pd.DataFrame(encoder.inverse_transform(model_prediction), columns=[\"Predictions\"])\n",
    "model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation \n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(tuned_model, x_scaled, y, cv=skfold)\n",
    "print(scores)\n",
    "\n",
    "#The accuracy is ranging from a minimum of 57% and a maximum of 65%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09d04b",
   "metadata": {},
   "source": [
    "#### The final model is saved in the current working directory as \"Logistic_model.sav\". The model predicts the data with a minimum of 57% accuracy and a maximum of 65% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc= DecisionTreeClassifier()\n",
    "dc.fit(x_train,y_train)\n",
    "\n",
    "dc_performance = performance(x_train,x_test,y_train,y_test,dc)\n",
    "dc_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(dc, x_scaled, y, cv=skfold)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfdff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
